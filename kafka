1.) MySQL Table (Table should have some column like created_at or updated_at so that can be used for incremental read)
create table db.delivery (
id int,
date_order date,
items varchar(50) ,
created_at datetime
);

insert into db.delivery values (1,'2023-04-04', 'Pav Bhaji','2023-04-04 16:00:00');
insert into db.delivery values (2,'2023-04-04','Panipuri','2023-04-04 16:10:00');
insert into db.delivery values (3,'2023-04-04','Chole Bhature','2023-04-04 16:11:00');
insert into db.delivery values (4,'2023-04-04','Sushi','2023-04-04 16:20:00');
insert into db.delivery values (5,'2023-04-04','Noodles','2023-04-04 16:25:00');
insert into db.delivery values (6,'2023-04-04','Biryani','2023-04-04 16:30:00');

2.) Write a python script which is running in infinite loop and inserting 4-5 dummy/dynamically prepared records
    in MySQL Table
    import mysql.connector
mydb = mysql.connector.connect(
    host = "localhost",
    username = "root",
    password = "user",
    database = "db"
)
mycursor = mydb.cursor()

 
sql = "INSERT INTO db.delivery VALUES (%s, %s)"
for i in range(1,15):
    val = (7,'2023-04-04', 'Milkshake','2023-04-04 16:00:00');
    mycursor.execute(sql, val)

    mydb.commit()


    mydb.close()

    mycursor.execute("select * from db.delivery")
    result = mycursor.fetchall()
    result.to_csv('/confluent-kafka/Data.csv')
3.) Setup Confluent Kafka
4.) Create Topic
5.) Create json schema on schema registry (depends on what kind of data you are publishing in mysql table)
Consumer:
    {
  "$id": "http://example.com/myURI.schema.json",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "additionalProperties": false,
  "description": "Sample schema to help you get started.",
  "properties": {
    "order_date": {
      "description": "The type(v) type is used.",
      "type": "string"
    },
    "items": {
      "description": "The type(v) type is used.",
      "type": "string"
    },
    "created_at": {
      "description": "The type(v) type is used.",
      "type": "string"
    }
 
  },
  "title": "SampleRecord",
  "type": "object"
}
6.) Write a producer code which will read the data from MySQL table incrementally (hint : use and maintain create_at column)

7.) Producer will publish data in Kafka Topic
8.) Write consumer group to consume data from Kafka topic
9.) In Kafka consumer code do some changes or transformation for each record and write it in Cassandra table
